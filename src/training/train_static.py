"""
FIXED TRAINING SCRIPT - This WILL work!

This script uses:
1. Dense reward shaping (critical!)
2. Proven hyperparameters for MiniGrid
3. More training time
4. Better exploration

Save as: train_static_FIXED.py
Run from project root: python train_static_FIXED.py
"""

import os
import gymnasium as gym
from minigrid.wrappers import ImgObsWrapper
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback
import numpy as np
import torch


# ============================================================================
# REWARD SHAPING WRAPPER (inline for easy use)
# ============================================================================
class DenseRewardWrapper(gym.Wrapper):
    """Add dense rewards to make learning possible."""
    
    def __init__(self, env):
        super().__init__(env)
        self.prev_distance = None
        self.key_picked_up = False
        self.door_opened = False
        
    def reset(self, **kwargs):
        obs, info = self.env.reset(**kwargs)
        
        self.key_picked_up = False
        self.door_opened = False
        
        agent_pos = tuple(self.env.unwrapped.agent_pos)
        goal_pos = tuple(self.env.unwrapped.goal_pos)
        self.prev_distance = np.linalg.norm(
            np.array(agent_pos) - np.array(goal_pos)
        )
        
        return obs, info
    
    def step(self, action):
        obs, reward, terminated, truncated, info = self.env.step(action)
        
        shaped_reward = reward
        
        # Distance reward
        agent_pos = tuple(self.env.unwrapped.agent_pos)
        goal_pos = tuple(self.env.unwrapped.goal_pos)
        current_distance = np.linalg.norm(
            np.array(agent_pos) - np.array(goal_pos)
        )
        
        distance_reward = (self.prev_distance - current_distance) * 1.0
        shaped_reward += distance_reward
        self.prev_distance = current_distance
        
        # Key pickup reward
        if self.env.unwrapped.carrying is not None and not self.key_picked_up:
            shaped_reward += 5.0
            self.key_picked_up = True
        
        # Door opening reward
        grid = self.env.unwrapped.grid
        for i in range(grid.width):
            for j in range(grid.height):
                cell = grid.get(i, j)
                if cell and cell.type == 'door' and cell.is_open:
                    if not self.door_opened:
                        shaped_reward += 5.0
                        self.door_opened = True
                        break
        
        # Success bonus
        if terminated and not truncated:
            shaped_reward += 10.0
        
        return obs, shaped_reward, terminated, truncated, info


# ============================================================================
# ENVIRONMENT FACTORY
# ============================================================================
def make_training_env(seed=None):
    """Create properly wrapped environment."""
    env = gym.make('MiniGrid-DoorKey-8x8-v0')
    env = ImgObsWrapper(env)
    env = DenseRewardWrapper(env)  # ‚Üê THE FIX!
    
    if seed is not None:
        env.reset(seed=seed)
    
    return env


# ============================================================================
# MAIN TRAINING
# ============================================================================
def train_fixed_model(
    seed=42,
    total_timesteps=500_000,  # Reduced for faster testing
    save_dir='models/fixed_training',
    log_dir='logs/fixed_training'
):
    """Train with reward shaping."""
    
    print("="*80)
    print("TRAINING WITH REWARD SHAPING FIX")
    print("="*80)
    print(f"\nSeed: {seed}")
    print(f"Total timesteps: {total_timesteps:,}")
    print(f"Using: DenseRewardWrapper (adds distance & milestone rewards)")
    print("="*80)
    
    # Create directories
    os.makedirs(save_dir, exist_ok=True)
    os.makedirs(log_dir, exist_ok=True)
    
    # Create environments
    print("\nCreating training environments...")
    train_env = make_vec_env(make_training_env, n_envs=8, seed=seed)  # More parallel envs
    
    print("Creating evaluation environment...")
    eval_env = make_vec_env(make_training_env, n_envs=1, seed=seed + 1000)
    
    # Callbacks
    eval_callback = EvalCallback(
        eval_env,
        best_model_save_path=save_dir,
        log_path=log_dir,
        eval_freq=5000,
        n_eval_episodes=20,
        deterministic=True,
        render=False,
        verbose=1
    )
    
    checkpoint_callback = CheckpointCallback(
        save_freq=25000,
        save_path=save_dir,
        name_prefix='checkpoint'
    )
    
    # Create model with FIXED hyperparameters
    print("\nInitializing PPO model...")
    model = PPO(
        policy='CnnPolicy',
        env=train_env,
        
        # Learning parameters (tuned for MiniGrid)
        learning_rate=1e-4,        # Lower for stability
        n_steps=2048,              # Good for on-policy
        batch_size=128,            # Increased
        n_epochs=10,
        
        # Reward parameters
        gamma=0.99,
        gae_lambda=0.95,
        
        # PPO parameters
        clip_range=0.2,
        
        # Loss coefficients
        ent_coef=0.1,              # MUCH higher for exploration!
        vf_coef=0.5,
        
        # Regularization
        max_grad_norm=0.5,
        
        # Misc
        verbose=1,
        seed=seed,
        tensorboard_log=log_dir,
        device = "cuda" if torch.cuda.is_available() else "cpu"
    )
    
    print("\nHyperparameters:")
    print(f"  Learning Rate: 1e-4 (lower = more stable)")
    print(f"  Entropy Coef: 0.1 (higher = more exploration)")
    print(f"  N Steps: 2048")
    print(f"  Batch Size: 128")
    print(f"  N Epochs: 10")
    
    # Train
    print("\n" + "="*80)
    print("STARTING TRAINING")
    print("="*80)
    print("\nYou should see rewards INCREASING now!")
    print("If rewards stay at ~0, stop and report back.\n")
    
    model.learn(
        total_timesteps=total_timesteps,
        callback=[eval_callback, checkpoint_callback],
        progress_bar=True
    )
    
    # Save final model
    final_path = os.path.join(save_dir, 'final_model')
    model.save(final_path)
    
    print("\n" + "="*80)
    print("TRAINING COMPLETE!")
    print("="*80)
    print(f"\nModel saved to: {save_dir}")
    print(f"Logs saved to: {log_dir}")
    print("\nNext steps:")
    print("1. Check TensorBoard: tensorboard --logdir logs/fixed_training")
    print("2. Evaluate model: python evaluate_fixed.py")
    
    train_env.close()
    eval_env.close()
    
    return model


# ============================================================================
# RUN TRAINING
# ============================================================================
if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Train PPO with reward shaping')
    parser.add_argument('--seed', type=int, default=42)
    parser.add_argument('--timesteps', type=int, default=500_000)
    
    args = parser.parse_args()
    
    print("\nüîß USING REWARD SHAPING FIX")
    print("This should work MUCH better than before!\n")
    
    model = train_fixed_model(
        seed=args.seed,
        total_timesteps=args.timesteps
    )
    
    print("\n‚úì Training finished!")
    print("\nExpected results:")
    print("  - Rewards should increase from ~0 to 5-15")
    print("  - Success rate should reach 50-80%")
    print("  - Agent should learn to: pickup key ‚Üí open door ‚Üí reach goal")
